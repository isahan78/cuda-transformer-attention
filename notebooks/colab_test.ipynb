{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# CUDA Transformer Attention - Google Colab Execution\n",
    "\n",
    "This notebook compiles and tests the CUDA attention kernels on Google Colab's GPU.\n",
    "\n",
    "## Setup Steps:\n",
    "1. **Enable GPU**: Runtime → Change runtime type → GPU (T4 or better)\n",
    "2. **Clone repository** (or upload files)\n",
    "3. **Compile CUDA extension**\n",
    "4. **Run tests and benchmarks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check-gpu"
   },
   "source": [
    "## Step 1: Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu-code"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ WARNING: GPU not available! Please enable GPU in Runtime settings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone-repo"
   },
   "source": [
    "## Step 2: Clone Repository\n",
    "\n",
    "Replace `<your_username>` with your GitHub username if you've pushed the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo-code"
   },
   "outputs": [],
   "source": [
    "# Option 1: Clone from GitHub\n",
    "# !git clone https://github.com/<your_username>/cuda-transformer-attention.git\n",
    "# %cd cuda-transformer-attention\n",
    "\n",
    "# Option 2: If files are already uploaded, just navigate to the directory\n",
    "# %cd cuda-transformer-attention\n",
    "\n",
    "# For testing, let's check if we're in the right directory\n",
    "!pwd\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install-deps"
   },
   "source": [
    "## Step 3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps-code"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pytest -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile-extension"
   },
   "source": [
    "## Step 4: Compile CUDA Extension\n",
    "\n",
    "This uses PyTorch's JIT compilation to build the C++/CUDA extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compile-extension-code"
   },
   "outputs": [],
   "source": [
    "from torch.utils.cpp_extension import load\n",
    "import os\n",
    "\n",
    "# Ensure we're in the project directory\n",
    "# os.chdir('/content/cuda-transformer-attention')  # Adjust path as needed\n",
    "\n",
    "print(\"Compiling CUDA extension... This may take 2-5 minutes.\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cuda_attn = load(\n",
    "    name=\"cuda_attn\",\n",
    "    sources=[\n",
    "        \"cuda/attention_qk.cu\",\n",
    "        \"cuda/attention_softmax.cu\",\n",
    "        \"cuda/attention_av.cu\",\n",
    "        \"cuda/attention_fused.cu\",\n",
    "        \"cpp/attention_binding.cpp\"\n",
    "    ],\n",
    "    extra_cuda_cflags=[\n",
    "        \"-O3\",\n",
    "        \"--use_fast_math\",\n",
    "        \"-std=c++14\"\n",
    "    ],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"✅ Compilation successful!\")\n",
    "print(f\"Extension module: {cuda_attn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-basic"
   },
   "source": [
    "## Step 5: Basic Functionality Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-basic-code"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from python.reference_attention import reference_attention\n",
    "from python.cuda_attention import cuda_attention_forward\n",
    "\n",
    "# Create test inputs\n",
    "B, H, S, D = 2, 4, 128, 64\n",
    "Q = torch.randn(B, H, S, D, device='cuda')\n",
    "K = torch.randn(B, H, S, D, device='cuda')\n",
    "V = torch.randn(B, H, S, D, device='cuda')\n",
    "\n",
    "print(f\"Test configuration: B={B}, H={H}, S={S}, D={D}\")\n",
    "print(\"\\nTesting different kernel modes...\\n\")\n",
    "\n",
    "# Test reference\n",
    "output_ref = reference_attention(Q, K, V)\n",
    "print(f\"✓ Reference:    shape={output_ref.shape}\")\n",
    "\n",
    "# Test CUDA kernels\n",
    "for mode in ['naive', 'tiled', 'fused']:\n",
    "    output = cuda_attention_forward(Q, K, V, mode=mode)\n",
    "    \n",
    "    # Check correctness\n",
    "    max_diff = (output - output_ref).abs().max().item()\n",
    "    mean_diff = (output - output_ref).abs().mean().item()\n",
    "    \n",
    "    print(f\"✓ {mode.capitalize():10s}: shape={output.shape}, \"\n",
    "          f\"max_diff={max_diff:.2e}, mean_diff={mean_diff:.2e}\")\n",
    "\n",
    "print(\"\\n✅ All modes working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-causal"
   },
   "source": [
    "## Step 6: Test Causal Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-causal-code"
   },
   "outputs": [],
   "source": [
    "print(\"Testing causal (autoregressive) attention...\\n\")\n",
    "\n",
    "output_ref_causal = reference_attention(Q, K, V, is_causal=True)\n",
    "print(f\"✓ Reference (causal):  shape={output_ref_causal.shape}\")\n",
    "\n",
    "for mode in ['naive', 'tiled', 'fused']:\n",
    "    output = cuda_attention_forward(Q, K, V, mode=mode, is_causal=True)\n",
    "    max_diff = (output - output_ref_causal).abs().max().item()\n",
    "    mean_diff = (output - output_ref_causal).abs().mean().item()\n",
    "    \n",
    "    print(f\"✓ {mode.capitalize():10s} (causal): max_diff={max_diff:.2e}, mean_diff={mean_diff:.2e}\")\n",
    "\n",
    "print(\"\\n✅ Causal masking working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "benchmark"
   },
   "source": [
    "## Step 7: Performance Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "benchmark-code"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark(func, *args, warmup=5, repeat=20, **kwargs):\n",
    "    \"\"\"Benchmark a function with CUDA synchronization.\"\"\"\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        func(*args, **kwargs)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Timing\n",
    "    times = []\n",
    "    for _ in range(repeat):\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        start.record()\n",
    "        func(*args, **kwargs)\n",
    "        end.record()\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        times.append(start.elapsed_time(end))\n",
    "    \n",
    "    return sum(times) / len(times), min(times), max(times)\n",
    "\n",
    "# Benchmark configuration\n",
    "configs = [\n",
    "    (4, 8, 256, 64, \"Small\"),\n",
    "    (4, 8, 512, 64, \"Medium\"),\n",
    "    (2, 8, 1024, 64, \"Large\"),\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" \"*25 + \"PERFORMANCE BENCHMARK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for B, H, S, D, name in configs:\n",
    "    print(f\"\\n{name}: B={B}, H={H}, S={S}, D={D}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Create inputs\n",
    "    Q = torch.randn(B, H, S, D, device='cuda')\n",
    "    K = torch.randn(B, H, S, D, device='cuda')\n",
    "    V = torch.randn(B, H, S, D, device='cuda')\n",
    "    \n",
    "    # Benchmark reference\n",
    "    mean_t, min_t, max_t = benchmark(reference_attention, Q, K, V)\n",
    "    ref_time = mean_t\n",
    "    print(f\"Reference:  {mean_t:7.3f} ms  (min: {min_t:6.3f}, max: {max_t:6.3f})\")\n",
    "    \n",
    "    # Benchmark CUDA modes\n",
    "    for mode in ['naive', 'tiled', 'fused']:\n",
    "        mean_t, min_t, max_t = benchmark(cuda_attention_forward, Q, K, V, mode=mode)\n",
    "        speedup = ref_time / mean_t if mean_t > 0 else 0\n",
    "        print(f\"{mode.capitalize():10s}:  {mean_t:7.3f} ms  (min: {min_t:6.3f}, max: {max_t:6.3f})  \"\n",
    "              f\"Speedup: {speedup:.2f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run-tests"
   },
   "source": [
    "## Step 8: Run Test Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-tests-code"
   },
   "outputs": [],
   "source": [
    "# Run correctness tests\n",
    "print(\"Running correctness tests...\\n\")\n",
    "!pytest tests/test_correctness.py -v --tb=short\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Running mask tests...\\n\")\n",
    "!pytest tests/test_masks.py -v --tb=short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "memory-test"
   },
   "source": [
    "## Step 9: Memory Usage Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "memory-test-code"
   },
   "outputs": [],
   "source": [
    "print(\"Memory Usage Comparison\\n\" + \"=\"*80)\n",
    "\n",
    "B, H, S, D = 2, 8, 1024, 64\n",
    "Q = torch.randn(B, H, S, D, device='cuda')\n",
    "K = torch.randn(B, H, S, D, device='cuda')\n",
    "V = torch.randn(B, H, S, D, device='cuda')\n",
    "\n",
    "print(f\"Configuration: B={B}, H={H}, S={S}, D={D}\\n\")\n",
    "\n",
    "for mode in ['naive', 'tiled', 'fused']:\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    start_mem = torch.cuda.memory_allocated()\n",
    "    output = cuda_attention_forward(Q, K, V, mode=mode)\n",
    "    torch.cuda.synchronize()\n",
    "    peak_mem = torch.cuda.max_memory_allocated()\n",
    "    \n",
    "    mem_used = (peak_mem - start_mem) / (1024**2)  # MB\n",
    "    print(f\"{mode.capitalize():10s}: {mem_used:8.2f} MB\")\n",
    "    \n",
    "    del output\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Note: Fused kernel should use less memory for large sequences\")\n",
    "print(\"as it avoids materializing the full attention matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✅ Successful compilation of CUDA kernels on Google Colab\n",
    "2. ✅ Correctness validation against PyTorch reference\n",
    "3. ✅ Causal masking support\n",
    "4. ✅ Performance benchmarking\n",
    "5. ✅ Memory usage comparison\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Naive kernel**: Simpler but slower, good for debugging\n",
    "- **Tiled kernel**: Better performance through shared memory optimization\n",
    "- **Fused kernel**: FlashAttention-style, memory-efficient for long sequences\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different configurations\n",
    "- Test on your own data\n",
    "- Compare with PyTorch's native attention\n",
    "- Profile with Nsight Systems for detailed analysis"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
