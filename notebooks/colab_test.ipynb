{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# CUDA Transformer Attention - Google Colab\n",
    "\n",
    "‚ö†Ô∏è **FIRST: Enable GPU!**\n",
    "- Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí Save\n",
    "\n",
    "Then run all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"‚ùå GPU not enabled! Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone"
   },
   "outputs": [],
   "source": [
    "# Clone repo (only if not already cloned)\n",
    "import os\n",
    "if not os.path.exists('/content/cuda-transformer-attention'):\n",
    "    !git clone https://github.com/isahan78/cuda-transformer-attention.git /content/cuda-transformer-attention\n",
    "else:\n",
    "    print(\"Repository already cloned\")\n",
    "\n",
    "%cd /content/cuda-transformer-attention\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "!pip install pytest ninja -q\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compile"
   },
   "outputs": [],
   "source": "from torch.utils.cpp_extension import load\nimport os, shutil\n\n# Set CUDA arch\nos.environ['TORCH_CUDA_ARCH_LIST'] = '7.5;8.0;8.6;8.9'\n\n# Clear cache\ncache = os.path.expanduser('~/.cache/torch_extensions')\nif os.path.exists(cache):\n    shutil.rmtree(cache, ignore_errors=True)\n\nprint(\"üî® Compiling CUDA extension... (~2-5 min)\\n\" + \"=\"*70)\n\ncuda_attn = load(\n    name=\"cuda_attn\",\n    sources=[\n        \"cuda/attention_qk.cu\",\n        \"cuda/attention_softmax.cu\",\n        \"cuda/attention_av.cu\",\n        \"cuda/attention_fused.cu\",\n        \"cpp/attention_binding.cpp\"\n    ],\n    extra_cuda_cflags=[\"-O2\", \"-std=c++17\", \"-D_GLIBCXX_USE_CXX11_ABI=0\"],\n    extra_cflags=[\"-O2\", \"-std=c++17\", \"-D_GLIBCXX_USE_CXX11_ABI=0\"],\n    verbose=True\n)\n\nprint(\"=\"*70 + \"\\n‚úÖ Compilation successful!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from python.reference_attention import reference_attention\n",
    "from python.cuda_attention import cuda_attention_forward\n",
    "\n",
    "B, H, S, D = 2, 4, 128, 64\n",
    "Q = torch.randn(B, H, S, D, device='cuda')\n",
    "K = torch.randn(B, H, S, D, device='cuda')\n",
    "V = torch.randn(B, H, S, D, device='cuda')\n",
    "\n",
    "print(f\"Test: B={B}, H={H}, S={S}, D={D}\\n\")\n",
    "\n",
    "output_ref = reference_attention(Q, K, V)\n",
    "print(f\"‚úì Reference: {output_ref.shape}\")\n",
    "\n",
    "for mode in ['naive', 'tiled', 'fused']:\n",
    "    output = cuda_attention_forward(Q, K, V, mode=mode)\n",
    "    diff = (output - output_ref).abs().max().item()\n",
    "    print(f\"‚úì {mode.capitalize():10s}: {output.shape}, diff={diff:.2e}\")\n",
    "\n",
    "print(\"\\n‚úÖ All kernels working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bench"
   },
   "outputs": [],
   "source": [
    "def bench(func, *args, **kwargs):\n",
    "    for _ in range(5): func(*args, **kwargs)\n",
    "    torch.cuda.synchronize()\n",
    "    times = []\n",
    "    for _ in range(20):\n",
    "        s = torch.cuda.Event(enable_timing=True)\n",
    "        e = torch.cuda.Event(enable_timing=True)\n",
    "        s.record(); func(*args, **kwargs); e.record()\n",
    "        torch.cuda.synchronize()\n",
    "        times.append(s.elapsed_time(e))\n",
    "    return sum(times) / len(times)\n",
    "\n",
    "B, H, S, D = 4, 8, 512, 64\n",
    "Q = torch.randn(B, H, S, D, device='cuda')\n",
    "K = torch.randn(B, H, S, D, device='cuda')\n",
    "V = torch.randn(B, H, S, D, device='cuda')\n",
    "\n",
    "print(f\"\\nBenchmark: B={B}, H={H}, S={S}, D={D}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ref = bench(reference_attention, Q, K, V)\n",
    "print(f\"Reference:  {ref:7.3f} ms\")\n",
    "\n",
    "for mode in ['naive', 'tiled', 'fused']:\n",
    "    t = bench(cuda_attention_forward, Q, K, V, mode=mode)\n",
    "    print(f\"{mode.capitalize():10s}:  {t:7.3f} ms  ({ref/t:.2f}x speedup)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "done"
   },
   "source": [
    "## ‚úÖ Done!\n",
    "\n",
    "All CUDA kernels compiled and tested successfully. You can now:\n",
    "- Test with your own data\n",
    "- Run full test suite: `!pytest tests/ -v`\n",
    "- Experiment with different sequence lengths"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}