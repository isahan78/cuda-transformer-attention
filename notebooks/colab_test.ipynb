{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# CUDA Transformer Attention - Google Colab\n",
    "\n",
    "âš ï¸ **FIRST: Enable GPU!**\n",
    "- Runtime â†’ Change runtime type â†’ GPU â†’ Save\n",
    "\n",
    "Then run all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"âŒ GPU not enabled! Runtime â†’ Change runtime type â†’ GPU\")\n",
    "print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone"
   },
   "outputs": [],
   "source": [
    "# Clone repo (only if not already cloned)\n",
    "import os\n",
    "if not os.path.exists('/content/cuda-transformer-attention'):\n",
    "    !git clone https://github.com/isahan78/cuda-transformer-attention.git /content/cuda-transformer-attention\n",
    "else:\n",
    "    print(\"Repository already cloned\")\n",
    "\n",
    "%cd /content/cuda-transformer-attention\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "!pip install pytest ninja -q\n",
    "print(\"âœ… Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compile"
   },
   "outputs": [],
   "source": "import os\n\n# Set CUDA arch for common Colab GPUs (T4, V100, A100)\nos.environ['TORCH_CUDA_ARCH_LIST'] = '7.5;8.0;8.6'\n\nprint(\"ðŸ”¨ Compiling CUDA extension... (~2-5 min)\\n\" + \"=\"*70)\n\n# Use setuptools build instead of JIT - more reliable in Colab\n!python setup.py build_ext --inplace 2>&1 | tail -20\n\n# Import the compiled extension\nimport cuda_attn\n\nprint(\"=\"*70 + \"\\nâœ… Compilation successful!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from python.reference_attention import reference_attention\n",
    "from python.cuda_attention import cuda_attention_forward\n",
    "\n",
    "B, H, S, D = 2, 4, 128, 64\n",
    "Q = torch.randn(B, H, S, D, device='cuda')\n",
    "K = torch.randn(B, H, S, D, device='cuda')\n",
    "V = torch.randn(B, H, S, D, device='cuda')\n",
    "\n",
    "print(f\"Test: B={B}, H={H}, S={S}, D={D}\\n\")\n",
    "\n",
    "output_ref = reference_attention(Q, K, V)\n",
    "print(f\"âœ“ Reference: {output_ref.shape}\")\n",
    "\n",
    "for mode in ['naive', 'tiled', 'fused']:\n",
    "    output = cuda_attention_forward(Q, K, V, mode=mode)\n",
    "    diff = (output - output_ref).abs().max().item()\n",
    "    print(f\"âœ“ {mode.capitalize():10s}: {output.shape}, diff={diff:.2e}\")\n",
    "\n",
    "print(\"\\nâœ… All kernels working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bench"
   },
   "outputs": [],
   "source": [
    "def bench(func, *args, **kwargs):\n",
    "    for _ in range(5): func(*args, **kwargs)\n",
    "    torch.cuda.synchronize()\n",
    "    times = []\n",
    "    for _ in range(20):\n",
    "        s = torch.cuda.Event(enable_timing=True)\n",
    "        e = torch.cuda.Event(enable_timing=True)\n",
    "        s.record(); func(*args, **kwargs); e.record()\n",
    "        torch.cuda.synchronize()\n",
    "        times.append(s.elapsed_time(e))\n",
    "    return sum(times) / len(times)\n",
    "\n",
    "B, H, S, D = 4, 8, 512, 64\n",
    "Q = torch.randn(B, H, S, D, device='cuda')\n",
    "K = torch.randn(B, H, S, D, device='cuda')\n",
    "V = torch.randn(B, H, S, D, device='cuda')\n",
    "\n",
    "print(f\"\\nBenchmark: B={B}, H={H}, S={S}, D={D}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ref = bench(reference_attention, Q, K, V)\n",
    "print(f\"Reference:  {ref:7.3f} ms\")\n",
    "\n",
    "for mode in ['naive', 'tiled', 'fused']:\n",
    "    t = bench(cuda_attention_forward, Q, K, V, mode=mode)\n",
    "    print(f\"{mode.capitalize():10s}:  {t:7.3f} ms  ({ref/t:.2f}x speedup)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "done"
   },
   "source": [
    "## âœ… Done!\n",
    "\n",
    "All CUDA kernels compiled and tested successfully. You can now:\n",
    "- Test with your own data\n",
    "- Run full test suite: `!pytest tests/ -v`\n",
    "- Experiment with different sequence lengths"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}